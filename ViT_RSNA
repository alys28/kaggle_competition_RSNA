{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import OneHotEncoder\nimport torch\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     print(dirname)\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-07-11T20:58:29.014203Z","iopub.execute_input":"2024-07-11T20:58:29.014629Z","iopub.status.idle":"2024-07-11T20:58:29.020388Z","shell.execute_reply.started":"2024-07-11T20:58:29.014592Z","shell.execute_reply":"2024-07-11T20:58:29.019455Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# We have to predict one of the five lumbar spine degenerative conditions, and where it happens (L1/L2,...) + the severity","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:46:15.885908Z","iopub.status.idle":"2024-07-02T04:46:15.886319Z","shell.execute_reply.started":"2024-07-02T04:46:15.886107Z","shell.execute_reply":"2024-07-02T04:46:15.886125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"%pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:58:31.975717Z","iopub.execute_input":"2024-07-11T20:58:31.976538Z","iopub.status.idle":"2024-07-11T20:58:44.054102Z","shell.execute_reply.started":"2024-07-11T20:58:31.976507Z","shell.execute_reply":"2024-07-11T20:58:44.052899Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv'\n\ntrain = pd.read_csv(train_path).dropna()\ntrain.head()\nx=train.iloc[:, 1:]\nlabel_enc = OneHotEncoder()\nsparse_matrix = label_enc.fit_transform(x)\nfeature_names = label_enc.get_feature_names_out(train.iloc[:, 1:].columns)\ndf = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=feature_names)\nprint(\"One-Hot Encoded Feature Names:\")\nprint(len(feature_names))\nprint(feature_names)\n# print(df)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:51:54.539863Z","iopub.execute_input":"2024-07-11T20:51:54.540695Z","iopub.status.idle":"2024-07-11T20:51:54.632599Z","shell.execute_reply.started":"2024-07-11T20:51:54.540655Z","shell.execute_reply":"2024-07-11T20:51:54.631688Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"One-Hot Encoded Feature Names:\n75\n['spinal_canal_stenosis_l1_l2_Moderate'\n 'spinal_canal_stenosis_l1_l2_Normal/Mild'\n 'spinal_canal_stenosis_l1_l2_Severe'\n 'spinal_canal_stenosis_l2_l3_Moderate'\n 'spinal_canal_stenosis_l2_l3_Normal/Mild'\n 'spinal_canal_stenosis_l2_l3_Severe'\n 'spinal_canal_stenosis_l3_l4_Moderate'\n 'spinal_canal_stenosis_l3_l4_Normal/Mild'\n 'spinal_canal_stenosis_l3_l4_Severe'\n 'spinal_canal_stenosis_l4_l5_Moderate'\n 'spinal_canal_stenosis_l4_l5_Normal/Mild'\n 'spinal_canal_stenosis_l4_l5_Severe'\n 'spinal_canal_stenosis_l5_s1_Moderate'\n 'spinal_canal_stenosis_l5_s1_Normal/Mild'\n 'spinal_canal_stenosis_l5_s1_Severe'\n 'left_neural_foraminal_narrowing_l1_l2_Moderate'\n 'left_neural_foraminal_narrowing_l1_l2_Normal/Mild'\n 'left_neural_foraminal_narrowing_l1_l2_Severe'\n 'left_neural_foraminal_narrowing_l2_l3_Moderate'\n 'left_neural_foraminal_narrowing_l2_l3_Normal/Mild'\n 'left_neural_foraminal_narrowing_l2_l3_Severe'\n 'left_neural_foraminal_narrowing_l3_l4_Moderate'\n 'left_neural_foraminal_narrowing_l3_l4_Normal/Mild'\n 'left_neural_foraminal_narrowing_l3_l4_Severe'\n 'left_neural_foraminal_narrowing_l4_l5_Moderate'\n 'left_neural_foraminal_narrowing_l4_l5_Normal/Mild'\n 'left_neural_foraminal_narrowing_l4_l5_Severe'\n 'left_neural_foraminal_narrowing_l5_s1_Moderate'\n 'left_neural_foraminal_narrowing_l5_s1_Normal/Mild'\n 'left_neural_foraminal_narrowing_l5_s1_Severe'\n 'right_neural_foraminal_narrowing_l1_l2_Moderate'\n 'right_neural_foraminal_narrowing_l1_l2_Normal/Mild'\n 'right_neural_foraminal_narrowing_l1_l2_Severe'\n 'right_neural_foraminal_narrowing_l2_l3_Moderate'\n 'right_neural_foraminal_narrowing_l2_l3_Normal/Mild'\n 'right_neural_foraminal_narrowing_l2_l3_Severe'\n 'right_neural_foraminal_narrowing_l3_l4_Moderate'\n 'right_neural_foraminal_narrowing_l3_l4_Normal/Mild'\n 'right_neural_foraminal_narrowing_l3_l4_Severe'\n 'right_neural_foraminal_narrowing_l4_l5_Moderate'\n 'right_neural_foraminal_narrowing_l4_l5_Normal/Mild'\n 'right_neural_foraminal_narrowing_l4_l5_Severe'\n 'right_neural_foraminal_narrowing_l5_s1_Moderate'\n 'right_neural_foraminal_narrowing_l5_s1_Normal/Mild'\n 'right_neural_foraminal_narrowing_l5_s1_Severe'\n 'left_subarticular_stenosis_l1_l2_Moderate'\n 'left_subarticular_stenosis_l1_l2_Normal/Mild'\n 'left_subarticular_stenosis_l1_l2_Severe'\n 'left_subarticular_stenosis_l2_l3_Moderate'\n 'left_subarticular_stenosis_l2_l3_Normal/Mild'\n 'left_subarticular_stenosis_l2_l3_Severe'\n 'left_subarticular_stenosis_l3_l4_Moderate'\n 'left_subarticular_stenosis_l3_l4_Normal/Mild'\n 'left_subarticular_stenosis_l3_l4_Severe'\n 'left_subarticular_stenosis_l4_l5_Moderate'\n 'left_subarticular_stenosis_l4_l5_Normal/Mild'\n 'left_subarticular_stenosis_l4_l5_Severe'\n 'left_subarticular_stenosis_l5_s1_Moderate'\n 'left_subarticular_stenosis_l5_s1_Normal/Mild'\n 'left_subarticular_stenosis_l5_s1_Severe'\n 'right_subarticular_stenosis_l1_l2_Moderate'\n 'right_subarticular_stenosis_l1_l2_Normal/Mild'\n 'right_subarticular_stenosis_l1_l2_Severe'\n 'right_subarticular_stenosis_l2_l3_Moderate'\n 'right_subarticular_stenosis_l2_l3_Normal/Mild'\n 'right_subarticular_stenosis_l2_l3_Severe'\n 'right_subarticular_stenosis_l3_l4_Moderate'\n 'right_subarticular_stenosis_l3_l4_Normal/Mild'\n 'right_subarticular_stenosis_l3_l4_Severe'\n 'right_subarticular_stenosis_l4_l5_Moderate'\n 'right_subarticular_stenosis_l4_l5_Normal/Mild'\n 'right_subarticular_stenosis_l4_l5_Severe'\n 'right_subarticular_stenosis_l5_s1_Moderate'\n 'right_subarticular_stenosis_l5_s1_Normal/Mild'\n 'right_subarticular_stenosis_l5_s1_Severe']\n","output_type":"stream"}]},{"cell_type":"code","source":"import pydicom\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\n# Step 2: Define paths to the DICOM files and CSV file\nroot = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification'\ntrain_csv = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv'\ndicom_main_dir = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\ntrain_series_desc_dir = root + '/train_series_descriptions.csv'\n# Step 3: Read the labels from the CSV file and one-hot encode them\ntrain_df = pd.read_csv(train_csv).fillna(\"Normal/Mild\")\nx = train_df.iloc[:, 1:]\ntrain_series_desc = pd.read_csv(train_series_desc_dir)\n\nlabel_enc = OneHotEncoder()\nsparse_matrix = label_enc.fit_transform(x)\ndense_matrix = sparse_matrix.toarray()  # Convert sparse matrix to dense numpy array\nall_labels = torch.tensor(dense_matrix, dtype=torch.float32).view(-1, 25, 3)  # Convert to PyTorch tensor\n# Step 4: Define a custom dataset class\nclass MRIDataset(Dataset):\n    def __init__(self, df, df_desc, dicom_main_dir, labels, w=256,h=256,max_length = 195, transform=None):\n        self.df = df\n        self.df_desc = df_desc\n        self.dicom_main_dir = dicom_main_dir\n        self.labels = labels\n        self.transform = transform\n        self.max_length = max_length\n        self.w=w\n        self.h=h\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_list = [[],[],[]]\n        index = {'Axial T2': 0, 'Sagittal T2/STIR': 1, 'Sagittal T1': 2 }\n        study_id = self.df.iloc[idx][\"study_id\"]\n        for _, series_dirs, _ in os.walk(os.path.join(self.dicom_main_dir, str(study_id))):\n            for series in series_dirs:\n                observations = self.df_desc[(self.df_desc[\"study_id\"] == study_id) & (self.df_desc[\"series_id\"] == np.int64(series))]\n                if observations.empty:\n                    continue\n                observations = observations.iloc[0]\n                description = observations[\"series_description\"]\n                path = os.listdir(os.path.join(self.dicom_main_dir, str(study_id), series))\n                for file in path:\n                    # All of the files in this loop should be in the same channel\n                    dicom_data = pydicom.dcmread(os.path.join(self.dicom_main_dir, str(study_id), series, file))\n                    image = dicom_data.pixel_array.astype(np.float32)\n                    image = (image - np.min(image)) / (np.max(image) - np.min(image))  # Normalize to [0, 1]\n                    if self.transform:\n                        image = self.transform(image)\n                        img_list[index[description]].append(image.squeeze())\n            break\n        padded_img_list, attention_masks = [], []\n        for i in range(len(img_list)):\n            padded_sequence, mask = self.add_padding_and_convert_to_torch(img_list[i], self.max_length, self.w, self.h)\n            padded_img_list.append(padded_sequence)\n            attention_masks.append(mask)\n        label = self.labels[idx]\n        padded_img_list = torch.stack(padded_img_list, dim=0)\n        attention_masks = torch.stack(attention_masks, dim=0)\n\n        return padded_img_list, attention_masks, label\n\n    def add_padding_and_convert_to_torch(self, lst, max_length, w, h):\n        if len(lst) < max_length:\n            dim = (w,h)\n            zero_matrix = torch.zeros(dim)\n            mask = [1] * len(lst) + [0] * (max_length - len(lst))\n            while len(lst) < max_length:\n                lst.append(zero_matrix)\n        else:\n            lst = lst[:max_length]\n            mask = [1] * max_length\n        lst = torch.stack(lst, dim=0)\n        mask = torch.tensor(mask, dtype=torch.long)\n        return lst, mask\n#         except Exception as e:\n#             print(\"Error: \", e)\n#             print(\"Study ID: \", study_id)\n#             print(\"Index: \", i)\n#             print(\"Image List:\", lst)\n#             return [], []\n# Define transformations (if needed)\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert to tensor\n    transforms.Resize((256, 256), antialias=None),\n    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n])\n\n\ndef display_one_study(dataloader):\n    for batch in dataloader:\n        print(batch[0].size())\n        print(batch[1].size())\n        print(batch[2].size())\n#         for idx, series_data in batch:\n#             label = labels[idx]        \n#             for series in series_data:\n#                 images = series[\"images\"]\n#                 series_id = series[\"series_id\"]\n#                 description = series[\"description\"]\n#                 print(f\"Series ID: {series_id}\")\n#                 print(f\"Description: {description}\")\n\n#                 num_images = len(images)\n#                 num_cols = 4  # Number of columns in the grid\n#                 num_rows = (num_images + num_cols - 1) // num_cols  # Compute number of rows\n#                 plt.figure(figsize=(num_cols * 4, num_rows * 4))\n#                 for i, image in enumerate(images):\n#                     image = image.squeeze()  # This will remove the extra dimensions\n#                     plt.subplot(num_rows, num_cols, i + 1)\n#                     plt.imshow(image, cmap='gray')\n#                     plt.axis('off')\n#     #             plt.suptitle(f'Label: {label.item()}')  # Display the label as the title\n#                 plt.show()\n\n            # break  # Only show one study for illustration purposes\n          # Only iterate once for illustration purposes\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:51:59.427033Z","iopub.execute_input":"2024-07-11T20:51:59.427448Z","iopub.status.idle":"2024-07-11T20:52:01.253946Z","shell.execute_reply.started":"2024-07-11T20:51:59.427419Z","shell.execute_reply":"2024-07-11T20:52:01.253072Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_series_desc = pd.read_csv(train_series_desc_dir)\nprint(train_series_desc['study_id'].nunique())\ntrain = pd.read_csv(train_csv)\nprint(len(train))","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:52:14.486470Z","iopub.execute_input":"2024-07-11T20:52:14.486856Z","iopub.status.idle":"2024-07-11T20:52:14.522486Z","shell.execute_reply.started":"2024-07-11T20:52:14.486827Z","shell.execute_reply":"2024-07-11T20:52:14.521581Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1975\n1975\n","output_type":"stream"}]},{"cell_type":"code","source":"|","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Create the dataset and dataloader\ntrain_dataset = MRIDataset(train_df, train_series_desc, dicom_main_dir, all_labels, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n# display_one_study(train_loader)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:52:44.736162Z","iopub.execute_input":"2024-07-11T20:52:44.736504Z","iopub.status.idle":"2024-07-11T20:52:44.741639Z","shell.execute_reply.started":"2024-07-11T20:52:44.736479Z","shell.execute_reply":"2024-07-11T20:52:44.740613Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        print(\"GPU Training\")\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\ndevice = get_default_device()\ntorch.device('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:52:47.300763Z","iopub.execute_input":"2024-07-11T20:52:47.301117Z","iopub.status.idle":"2024-07-11T20:52:47.337324Z","shell.execute_reply.started":"2024-07-11T20:52:47.301088Z","shell.execute_reply":"2024-07-11T20:52:47.336290Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"GPU Training\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# Checking memory\nimport gc\n# # Manually run garbage collector\ngc.collect()\n\n# # Empty CUDA cache\ntorch.cuda.empty_cache()\nprint(f\"Allocated Memory before loading: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\nprint(f\"Reserved Memory before loading: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\ntrain_dataset = MRIDataset(train_df, train_series_desc, dicom_main_dir, all_labels, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n\nfor batch in train_loader:\n    images = batch[0]\n    labels = batch[2]\n    to_device(images, device)\n    to_device(labels, device)\n    print(images.size())\n    del images\n    del labels\n    break\n# Print memory usage\nprint(f\"Allocated Memory: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\nprint(f\"Reserved Memory: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\n\n# Delete tensors\n\n\n# # Manually run garbage collector\ngc.collect()\n\n# # Empty CUDA cache\ntorch.cuda.empty_cache()\n\n# Print memory usage after clearing\nprint(f\"Allocated Memory after clearing: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\nprint(f\"Reserved Memory after clearing: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T05:39:58.306077Z","iopub.execute_input":"2024-07-09T05:39:58.306444Z","iopub.status.idle":"2024-07-09T05:40:01.251865Z","shell.execute_reply.started":"2024-07-09T05:39:58.306415Z","shell.execute_reply":"2024-07-09T05:40:01.250855Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Allocated Memory before loading: 174.10 MB\nReserved Memory before loading: 2192.00 MB\ntorch.Size([1, 3, 195, 256, 256])\nAllocated Memory: 174.10 MB\nReserved Memory: 2192.00 MB\nAllocated Memory after clearing: 174.10 MB\nReserved Memory after clearing: 2192.00 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:55:59.685017Z","iopub.execute_input":"2024-07-02T05:55:59.686100Z","iopub.status.idle":"2024-07-02T05:55:59.880670Z","shell.execute_reply.started":"2024-07-02T05:55:59.686058Z","shell.execute_reply":"2024-07-02T05:55:59.879590Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"248"},"metadata":{}}]},{"cell_type":"markdown","source":"May need to add more processing of images, check distribution of data also\n(Data Augmentation?)","metadata":{}},{"cell_type":"code","source":"import os\ndef get_shortest_video(directory):\n    shortest = 1000000\n    for study in os.listdir(directory):\n        for series in os.listdir(os.path.join(directory, study)):\n            length = len(os.listdir(os.path.join(directory, study, series)))\n            if length < shortest:\n                shortest = length\n                print(os.path.join(directory, study), length)\n    return shortest\n\n# get_shortest_video(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images\")\n\nimport os\ndef get_longest_video(directory):\n    longest = 0\n    for study in os.listdir(directory):\n        for series in os.listdir(os.path.join(directory, study)):\n            length = len(os.listdir(os.path.join(directory, study, series)))\n            if length > longest:\n                longest = length\n                print(os.path.join(directory, study, series), length)\n    return longest\n\n# get_longest_video(\"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images\") # Shortest: 2","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:52:52.149375Z","iopub.execute_input":"2024-07-11T20:52:52.149748Z","iopub.status.idle":"2024-07-11T20:52:52.158855Z","shell.execute_reply.started":"2024-07-11T20:52:52.149720Z","shell.execute_reply":"2024-07-11T20:52:52.157681Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def display_from_study(directory):\n    images = []\n    titles = []\n\n    for series in os.listdir(directory):\n        series_path = os.path.join(directory, series)\n        if not os.path.isdir(series_path):\n            continue  # Skip if it's not a directory\n#         if not (series in ['300517765', '2097107888', '2679683906', '3114813181']):\n#             continue\n        for file in os.listdir(series_path):\n            print(os.listdir(series_path))\n            file_path = os.path.join(series_path, file)\n            dicom_data = pydicom.dcmread(file_path)\n            image = dicom_data.pixel_array.astype(np.float32)\n\n            images.append(image)\n            titles.append(f'Series: {series}, File: {file}')\n        \n    num_images = len(images)\n    num_cols = 4  # Number of columns in the grid\n    num_rows = (num_images + num_cols - 1) // num_cols  # Compute number of rows\n    plt.figure(figsize=(num_cols * 4, num_rows * 4))\n\n    for i, (image, title) in enumerate(zip(images, titles)):\n        plt.subplot(num_rows, num_cols, i + 1)\n        plt.imshow(image, cmap='gray')\n        plt.axis('off')\n    plt.show()\n\n\ndirectory = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/4096820034'\n# display_from_study(directory)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:52:55.096932Z","iopub.execute_input":"2024-07-11T20:52:55.097742Z","iopub.status.idle":"2024-07-11T20:52:55.106709Z","shell.execute_reply.started":"2024-07-11T20:52:55.097708Z","shell.execute_reply":"2024-07-11T20:52:55.105788Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nfrom einops import rearrange, repeat, reduce\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.norm = nn.LayerNorm(dim)\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        \n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x, mask=None):\n        x = self.norm(x)\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n#         print(dots.size())\n#         if exists(mask):\n#             mask = rearrange(mask, 'b n -> b 1 1 n')\n#             print(mask.size())\n#             dots = dots.masked_fill(mask == 0, float('-inf'))\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.layers = nn.ModuleList([])\n        \n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n                FeedForward(dim, mlp_dim, dropout = dropout)\n            ]))\n    def forward(self, x, mask=None):\n        for attn, ff in self.layers:\n            x = attn(x, mask=mask) + x\n            x = ff(x) + x\n        return self.norm(x)\n\nclass ViT(nn.Module):\n    def __init__(\n        self,\n        *,\n        image_size,\n        image_patch_size,\n        frames,\n        frame_patch_size,\n        num_classes,\n        dim,\n        spatial_depth,\n        temporal_depth,\n        heads,\n        mlp_dim,\n        pool = 'cls',\n        channels = 3,\n        dim_head = 64,\n        dropout = 0.,\n        emb_dropout = 0.\n    ):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(image_patch_size)\n        self.num_classes = num_classes\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n        assert frames % frame_patch_size == 0, 'Frames must be divisible by frame patch size'\n\n        num_image_patches = (image_height // patch_height) * (image_width // patch_width)\n        num_frame_patches = (frames // frame_patch_size)\n\n        patch_dim = channels * patch_height * patch_width * frame_patch_size\n\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.global_average_pool = pool == 'mean'\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (f pf) (h p1) (w p2) -> b f (h w) (p1 p2 pf c)', p1 = patch_height, p2 = patch_width, pf = frame_patch_size),\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim),\n            nn.LayerNorm(dim)\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_frame_patches, num_image_patches, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.spatial_cls_token = nn.Parameter(torch.randn(1, 1, dim)) if not self.global_average_pool else None\n        self.temporal_cls_token = nn.Parameter(torch.randn(1, 1, dim)) if not self.global_average_pool else None\n\n        self.spatial_transformer = Transformer(dim, spatial_depth, heads, dim_head, mlp_dim, dropout)\n        self.temporal_transformer = Transformer(dim, temporal_depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Linear(dim, num_classes)\n\n    def forward(self, video, mask=None):\n        x = self.to_patch_embedding(video)\n        b, f, n, _ = x.shape\n        x = x + self.pos_embedding[:, :f, :n]\n        if exists(self.spatial_cls_token):\n            spatial_cls_tokens = repeat(self.spatial_cls_token, '1 1 d -> b f 1 d', b = b, f = f)\n            x = torch.cat((spatial_cls_tokens, x), dim = 2)\n        \n        x = self.dropout(x)\n\n        x = rearrange(x, 'b f n d -> (b f) n d')\n        mask = rearrange(mask, 'b n f -> (b n) f')\n        # attend across space\n        x = self.spatial_transformer(x, mask=mask)\n        x = rearrange(x, '(b f) n d -> b f n d', b = b)\n\n        # excise out the spatial cls tokens or average pool for temporal attention\n\n        x = x[:, :, 0] if not self.global_average_pool else reduce(x, 'b f n d -> b f d', 'mean')\n\n        # append temporal CLS tokens\n\n        if exists(self.temporal_cls_token):\n            temporal_cls_tokens = repeat(self.temporal_cls_token, '1 1 d-> b 1 d', b = b)\n\n            x = torch.cat((temporal_cls_tokens, x), dim = 1)\n        \n        # attend across time\n\n        x = self.temporal_transformer(x)\n        # excise out temporal cls token or average pool\n\n        x = x[:, 0] if not self.global_average_pool else reduce(x, 'b f d -> b d', 'mean')\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:52:58.510496Z","iopub.execute_input":"2024-07-11T20:52:58.511045Z","iopub.status.idle":"2024-07-11T20:52:58.551819Z","shell.execute_reply.started":"2024-07-11T20:52:58.511017Z","shell.execute_reply":"2024-07-11T20:52:58.550958Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class VitLumbarSpine(nn.Module):\n    def __init__(self, vit, num_diseases=5, num_areas=5, num_classes=3):\n        super(VitLumbarSpine, self).__init__()\n        self.vit = vit\n        self.num_diseases = num_diseases\n        self.num_areas = num_areas\n        vit_output_dim = vit.num_classes\n\n        # Create separate output heads for each disease in each area\n        self.outputs = nn.ModuleList()\n        for _ in range(num_diseases * num_areas): # should be 25\n            self.outputs.append(nn.Linear(vit_output_dim, num_classes))\n    \n    def forward(self, x, mask=None):\n        features = self.vit(x, mask)\n        outputs = [output(features) for output in self.outputs]\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:53:03.976737Z","iopub.execute_input":"2024-07-11T20:53:03.977066Z","iopub.status.idle":"2024-07-11T20:53:03.984030Z","shell.execute_reply.started":"2024-07-11T20:53:03.977041Z","shell.execute_reply":"2024-07-11T20:53:03.983121Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ndef train(model, train_loader, criterion, optimizer, epochs=5, save_path='./vivit_checkpoints', save_freq=1, device=get_default_device()):\n    model.train()\n#     print(f\"Allocated Memory before loading: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\n#     print(f\"Reserved Memory before loading: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\n    os.makedirs(save_path, exist_ok=True)\n    for epoch in range(epochs):\n        running_loss = 0.0\n        # Initialize tqdm for progress bar\n        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n        for batch_idx, (images, masks, labels) in pbar:\n            optimizer.zero_grad()\n            images = to_device(images, device)\n            labels = to_device(labels, device)\n#             print(f\"Allocated Memory after loading new batch: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\n#             print(f\"Reserved Memory after loading new batch: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\n            # Forward pass\n            outputs = model(images, masks)\n#             print(f\"Allocated Memory after having run batch: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\n#             print(f\"Reserved Memory after run batch: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\n            # Compute the loss for each output\n            loss = 0\n            for i in range(len(outputs)):\n                loss += criterion(outputs[i], labels[:, i])\n            \n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            # Update tqdm progress bar\n            pbar.set_description(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/(batch_idx+1):.4f}\")\n#             print(f\"Allocated Memory after loss for batch: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\n#             print(f\"Reserved Memory after loss for batch: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\n        # Print epoch summary\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n        \n        # Save model checkpoint\n        if (epoch + 1) % save_freq == 0:\n            checkpoint = {\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': running_loss / len(train_loader)\n            }\n            checkpoint_path = os.path.join(save_path, f'checkpoint2_epoch_{epoch + 1}.pt')\n            torch.save(checkpoint, checkpoint_path)\n            print(f\"Checkpoint saved at {checkpoint_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:58:57.243429Z","iopub.execute_input":"2024-07-11T20:58:57.244414Z","iopub.status.idle":"2024-07-11T20:58:57.257025Z","shell.execute_reply.started":"2024-07-11T20:58:57.244376Z","shell.execute_reply":"2024-07-11T20:58:57.255740Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"GPU Training\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-09T05:38:52.096089Z","iopub.execute_input":"2024-07-09T05:38:52.096965Z","iopub.status.idle":"2024-07-09T05:38:52.101255Z","shell.execute_reply.started":"2024-07-09T05:38:52.096934Z","shell.execute_reply":"2024-07-09T05:38:52.100075Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"","metadata":{"execution":{"iopub.status.busy":"2024-07-02T04:55:01.405091Z","iopub.execute_input":"2024-07-02T04:55:01.405776Z","iopub.status.idle":"2024-07-02T04:55:01.409826Z","shell.execute_reply.started":"2024-07-02T04:55:01.405744Z","shell.execute_reply":"2024-07-02T04:55:01.408855Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import os\nprint(os.getenv('PYTORCH_CUDA_ALLOC_CONF'))  # Should output: 'max_split_size_mb:128'","metadata":{"execution":{"iopub.status.busy":"2024-07-05T00:28:33.614176Z","iopub.execute_input":"2024-07-05T00:28:33.614537Z","iopub.status.idle":"2024-07-05T00:28:33.619913Z","shell.execute_reply.started":"2024-07-05T00:28:33.614509Z","shell.execute_reply":"2024-07-05T00:28:33.618909Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Train the model\nvit_model = ViT(image_size=(256,256),\n        image_patch_size=16,\n        frames=195,\n        frame_patch_size=5,\n        num_classes=256,\n        dim=512,\n        spatial_depth=8,\n        temporal_depth=8,\n        heads=12,\n        mlp_dim=512,)\ninitial_memory = torch.cuda.memory_allocated(device)\nvit_model = to_device(vit_model, device)\nmodel = VitLumbarSpine(vit_model)\nmodel = to_device(model, device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nfinal_memory = torch.cuda.memory_allocated(device)\nmodel_memory = final_memory - initial_memory\nprint(f\"Memory allocated for the model: {model_memory / 1024**2:.2f} MB\")\nprint(f\"GPU used {torch.cuda.memory_allocated(device=get_default_device()) / 1024**2:.2f} MB of memory\")\n\ntrain(model, train_loader, criterion, optimizer, epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-11T20:59:00.751759Z","iopub.execute_input":"2024-07-11T20:59:00.752287Z","iopub.status.idle":"2024-07-11T20:59:05.228934Z","shell.execute_reply.started":"2024-07-11T20:59:00.752243Z","shell.execute_reply":"2024-07-11T20:59:05.227492Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Memory allocated for the model: 157.85 MB\nGPU Training\nGPU used 13316.17 MB of memory\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/988 [00:03<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMemory allocated for the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_memory\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU used \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(device\u001b[38;5;241m=\u001b[39mget_default_device())\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MB of memory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[18], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epochs, save_path, save_freq, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m             labels \u001b[38;5;241m=\u001b[39m to_device(labels, device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#             print(f\"Allocated Memory after loading new batch: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#             print(f\"Reserved Memory after loading new batch: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#             print(f\"Allocated Memory after having run batch: {torch.cuda.memory_allocated('cuda') / 1024**2:.2f} MB\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#             print(f\"Reserved Memory after run batch: {torch.cuda.memory_reserved('cuda') / 1024**2:.2f} MB\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;66;03m# Compute the loss for each output\u001b[39;00m\n\u001b[1;32m     22\u001b[0m             loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mVitLumbarSpine.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 15\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output(features) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 155\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, video, mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m mask \u001b[38;5;241m=\u001b[39m rearrange(mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n f -> (b n) f\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# attend across space\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatial_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(b f) n d -> b f n d\u001b[39m\u001b[38;5;124m'\u001b[39m, b \u001b[38;5;241m=\u001b[39m b)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# excise out the spatial cls tokens or average pool for temporal attention\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 82\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 82\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     83\u001b[0m         x \u001b[38;5;241m=\u001b[39m ff(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 56\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m         qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_qkv(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     54\u001b[0m         q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads), qkv)\n\u001b[0;32m---> 56\u001b[0m         dots \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#         print(dots.size())\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#         if exists(mask):\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#             mask = rearrange(mask, 'b n -> b 1 1 n')\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#             print(mask.size())\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#             dots = dots.masked_fill(mask == 0, float('-inf'))\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattend(dots)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 236.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 175.12 MiB is free. Process 3105 has 15.71 GiB memory in use. Of the allocated memory 15.18 GiB is allocated by PyTorch, and 241.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 236.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 175.12 MiB is free. Process 3105 has 15.71 GiB memory in use. Of the allocated memory 15.18 GiB is allocated by PyTorch, and 241.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2024-07-05T13:20:51.510963Z","iopub.execute_input":"2024-07-05T13:20:51.511400Z","iopub.status.idle":"2024-07-05T13:20:52.740638Z","shell.execute_reply.started":"2024-07-05T13:20:51.511350Z","shell.execute_reply":"2024-07-05T13:20:52.739086Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, test_loader):\n    model.eval()\n    correct = [0] * (num_diseases * num_areas)\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            outputs = model(images)\n            for i in range(len(outputs)):\n                _, predicted = torch.max(outputs[i], 1)\n                correct[i] += (predicted == labels[:, i]).sum().item()\n            total += labels.size(0)\n    \n    for i in range(len(correct)):\n        print(f'Accuracy of output {i} on the test images: {100 * correct[i] / total:.2f}%')\n# test(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T05:04:19.946307Z","iopub.execute_input":"2024-07-02T05:04:19.946695Z","iopub.status.idle":"2024-07-02T05:04:19.956174Z","shell.execute_reply.started":"2024-07-02T05:04:19.946666Z","shell.execute_reply":"2024-07-02T05:04:19.955259Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:50:50.079606Z","iopub.execute_input":"2024-07-01T20:50:50.081109Z","iopub.status.idle":"2024-07-01T20:50:51.211764Z","shell.execute_reply.started":"2024-07-01T20:50:50.080969Z","shell.execute_reply":"2024-07-01T20:50:51.209948Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd vivit_checkpoints/; pwd","metadata":{"execution":{"iopub.status.busy":"2024-07-01T20:51:03.651189Z","iopub.execute_input":"2024-07-01T20:51:03.652246Z","iopub.status.idle":"2024-07-01T20:51:04.786178Z","shell.execute_reply.started":"2024-07-01T20:51:03.652194Z","shell.execute_reply":"2024-07-01T20:51:04.784523Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"/kaggle/working/vivit_checkpoints\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}